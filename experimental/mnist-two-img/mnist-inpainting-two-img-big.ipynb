{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0TD5ZrvEMbhZ"
   },
   "source": [
    "# MNIST Inpainting con imagen de referencia\n",
    "En este experimento se implementará y entrenará un modelo de image inpainting sobre el dataset de MNIST, utilizando una segunda imágen como referencia. \n",
    "\n",
    "Se entrenará un modelo capaz de regenerar secciones faltantes dentro de imágenes de MNIST de forma realista. El modelo consiste de Deep Convolutional Networks, y se entrenará mediante un framework GAN, es decir, se entrenará un modelo generador y un discriminador en simultaneo.\n",
    "\n",
    "En esta versión, el generador recibe tanto la imágen con la sección a ser regenerada, como otra imagen de referencia del mismo dígito."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "## Imports y setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio in /home/gaston/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages (2.4.1)\r\n",
      "Requirement already satisfied: numpy in /home/gaston/.local/lib/python3.5/site-packages (from imageio) (1.14.3)\r\n",
      "Requirement already satisfied: pillow in /home/gaston/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages (from imageio) (5.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "# to generate gifs\n",
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YfIk2es3hJEd"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import time\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_DIR = '/home/gaston/workspace/two-face/two-face-inpainting-experiments/experimental/mnist-two-img/mnist-inpainting-two-img-big'\n",
    "VALIDATION_IMGS_DIR = os.path.join(EXPERIMENT_DIR, 'validation_imgs')\n",
    "if not os.path.exists(VALIDATION_IMGS_DIR):\n",
    "    os.makedirs(VALIDATION_IMGS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iYn4MdZnKCey"
   },
   "source": [
    "## Carga del dataset\n",
    "\n",
    "Usaremos el dataset de MNIST para entrenar al generador y al discriminador, el cual viene incluido dentro de la API de tf.keras. Del dataset de training se utiliza un 90% para el entrenamiento, y se separa un 10% para validación. En cada uno de estos datasets se separa un conjunto de imágenes para ser enmascaradas (las cuales seran regeneradas por el generador), y otro conjunto de imágenes para usar de referencia. A su vez, en el dataset de entrenamiento se separa un tercer conjunto de imágenes reales para entrenar al discriminador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 28\n",
    "PATCH_SIZE = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_fn(img_size, patch_size, use_batch=False):\n",
    "  patch_start = (img_size - patch_size) // 2\n",
    "  img_size_after_patch = img_size - (patch_start + patch_size)\n",
    "\n",
    "  def mask_fn(image, reference):\n",
    "    \"\"\"\n",
    "    Applies a mask of zeroes of size (patch_size x patch_size) at the center of the image.\n",
    "    Returns a tuple of the masked image and the original image.\n",
    "    \"\"\"\n",
    "    upper_edge = tf.ones([patch_start, img_size, 3], tf.float32)\n",
    "    lower_edge = tf.ones([img_size_after_patch, img_size, 3], tf.float32)\n",
    "\n",
    "    middle_left = tf.ones([patch_size, patch_start, 3], tf.float32)\n",
    "    middle_right = tf.ones([patch_size, img_size_after_patch, 3],\n",
    "                           tf.float32)\n",
    "\n",
    "    zeros = tf.zeros([patch_size, patch_size, 3], tf.float32)\n",
    "\n",
    "    middle = tf.concat([middle_left, zeros, middle_right], axis=1)\n",
    "    mask = tf.concat([upper_edge, middle, lower_edge], axis=0)\n",
    "\n",
    "    if use_batch:\n",
    "      mask = tf.expand_dims(mask, axis=0)\n",
    "\n",
    "    return (image * mask, image, reference)\n",
    "\n",
    "  return mask_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reference_dictionary(reference_labels):\n",
    "  \"\"\"\n",
    "  Returns a dictionary where each key corresponds to an MNIST label (0-9), \n",
    "  and each value is a list of the indeces from reference_labels containing the key label.\n",
    "  \"\"\"\n",
    "  reference_dict = {}\n",
    "  for label in range(10):\n",
    "    reference_dict[label] = np.where(reference_labels == label)[0]\n",
    "  return reference_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_references(reference_pool, labels, reference_dict):\n",
    "  reference_images = []\n",
    "  for label in labels:\n",
    "    reference_idx = np.random.choice(reference_dict[label])\n",
    "    reference_images.append(reference_pool[reference_idx])\n",
    "  return np.array(reference_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFC2ghIdiZYE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "# Normalize the images to [-1, 1]\n",
    "train_images = (train_images - 127.5) / 127.5\n",
    "test_images = (test_images - 127.5) / 127.5\n",
    "\n",
    "indeces = np.random.permutation(len(train_images))\n",
    "train_images = train_images[indeces]\n",
    "train_labels = train_labels[indeces]\n",
    "\n",
    "# Split between training and validation sets\n",
    "TRAINING_SAMPLES = int(len(train_images) * 0.9)\n",
    "train_set_images = train_images[:TRAINING_SAMPLES,:,:,:]\n",
    "validation_set_images = train_images[TRAINING_SAMPLES:,:,:,:]\n",
    "\n",
    "# TODO: generar diccionario de referencias para validation.\n",
    "\n",
    "train_set_labels = train_labels[:TRAINING_SAMPLES]\n",
    "validation_set_labels = train_labels[TRAINING_SAMPLES:]\n",
    "\n",
    "# Split training images between masked, full, and reference images.\n",
    "REAL_IMAGES_BOUNDRY = int(len(train_set_images) * 0.4)\n",
    "MASKED_IMAGES_BOUNDRY = REAL_IMAGES_BOUNDRY + int(len(train_set_images) * 0.4)\n",
    "\n",
    "train_full_images = train_set_images[:REAL_IMAGES_BOUNDRY,:,:,:]\n",
    "train_masked_images = train_set_images[REAL_IMAGES_BOUNDRY:MASKED_IMAGES_BOUNDRY,:,:,:]\n",
    "train_reference_images = train_set_images[MASKED_IMAGES_BOUNDRY:,:,:,:]\n",
    "\n",
    "train_full_labels = train_set_labels[:REAL_IMAGES_BOUNDRY]\n",
    "train_masked_labels = train_set_labels[REAL_IMAGES_BOUNDRY:MASKED_IMAGES_BOUNDRY]\n",
    "train_reference_labels = train_set_labels[MASKED_IMAGES_BOUNDRY:]\n",
    "\n",
    "\n",
    "train_reference_dict = create_reference_dictionary(train_reference_labels)\n",
    "train_full_references = get_references(train_reference_images, train_full_labels, train_reference_dict)\n",
    "train_masked_references = get_references(train_reference_images, train_masked_labels, train_reference_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-yKCCQOoJ7cn"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = TRAINING_SAMPLES\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_full_images_ds = tf.data.Dataset.from_tensor_slices((train_full_images, train_full_references)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "train_masked_images_ds = tf.data.Dataset.from_tensor_slices((train_masked_images, train_masked_references)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).map(get_mask_fn(IMAGE_SIZE, PATCH_SIZE, use_batch=True))\n",
    "\n",
    "train_dataset = tf.data.Dataset.zip((train_full_images_ds, train_masked_images_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_and_save_images(images, name):\n",
    "  fig = plt.figure(figsize=(8,8))\n",
    "  \n",
    "  for i in range(images.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(images[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "        \n",
    "  plt.savefig(name)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imágenes enmascaradas\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imágenes reales\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imágenes de referencia\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split validation images between masked and reference images.\n",
    "VALIDATION_IMAGES_BOUNDRY = int(len(validation_set_images) * 0.5)\n",
    "\n",
    "validation_images = validation_set_images[VALIDATION_IMAGES_BOUNDRY:,:,:,:]\n",
    "validation_reference_images = validation_set_images[:VALIDATION_IMAGES_BOUNDRY,:,:,:]\n",
    "\n",
    "validation_labels = validation_set_labels[VALIDATION_IMAGES_BOUNDRY:]\n",
    "validation_reference_labels = validation_set_labels[:VALIDATION_IMAGES_BOUNDRY]\n",
    "\n",
    "validation_reference_dict = create_reference_dictionary(validation_reference_labels)\n",
    "validation_images_references = get_references(validation_reference_images,\n",
    "                                              validation_labels, \n",
    "                                              validation_reference_dict)\n",
    "\n",
    "# Save a subset of validation images for displaying\n",
    "validation_images_subset = validation_images[:16]\n",
    "validation_references = validation_images_references[:16]\n",
    "\n",
    "validation_masked_images = []\n",
    "for i, image in enumerate(validation_images_subset):\n",
    "  mask_fn = get_mask_fn(IMAGE_SIZE, PATCH_SIZE, use_batch=False)\n",
    "  masked_image, _, _ = mask_fn(image, validation_references[i])\n",
    "  validation_masked_images.append(masked_image.numpy())\n",
    "\n",
    "validation_masked_images = np.array(validation_masked_images)\n",
    "\n",
    "print(\"Imágenes enmascaradas\")\n",
    "\n",
    "show_and_save_images(validation_masked_images, os.path.join(VALIDATION_IMGS_DIR, 'validation_masked_images.png'))\n",
    "\n",
    "print(\"Imágenes reales\")\n",
    "\n",
    "show_and_save_images(validation_images_subset, os.path.join(VALIDATION_IMGS_DIR, 'validation_unmasked_images.png'))\n",
    "\n",
    "print(\"Imágenes de referencia\")\n",
    "show_and_save_images(validation_references, os.path.join(VALIDATION_IMGS_DIR, 'validation_reference_images.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "## Creación de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-tEyxE-GMC48"
   },
   "source": [
    "### Generador\n",
    "\n",
    "El generador recibe como entrada dos imágenes: una imágen enmascarada de un dígito, y otra imágen de referencia de ese mismo dígito. Su objetivo es regenerar las secciones faltantes en las imágenes enmascaradas de forma realista, tomando como referencia la segunda imagen. \n",
    "\n",
    "La arquitectura del generador consiste de dos Encoders y un Decoder. Un Encoder recibe la imágen enmascarada, y el otro Encoder recibe la imágen de referencia. Cada enconder consiste de 3 capas convolucionales, con Batch Normalization y Leaky ReLU.\n",
    "\n",
    "Luego, los resultados de ambos encoders son concatenados y enviados al Decoder. El Decoder aplica 3 capas convolucionales transpuestas hasta generar una imágen del tamaño de la región faltante. Esta imágen será el output del generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "  masked_image = tf.keras.Input(shape=(28,28,1,), name='masked_image')\n",
    "  \n",
    "  masked_encoding = tf.keras.layers.Conv2D(64, (5, 5),\n",
    "                                                  strides=(1, 1), \n",
    "                                                  padding='same', \n",
    "                                                  input_shape=(28, 28, 1))(masked_image)\n",
    "  masked_encoding = tf.keras.layers.BatchNormalization()(masked_encoding)\n",
    "  masked_encoding = tf.keras.layers.LeakyReLU()(masked_encoding)\n",
    "  # 28x28x64\n",
    "    \n",
    "  masked_encoding = tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')(masked_encoding)\n",
    "  masked_encoding = tf.keras.layers.BatchNormalization()(masked_encoding)\n",
    "  masked_encoding = tf.keras.layers.LeakyReLU()(masked_encoding)\n",
    "  # 14x14x64\n",
    "\n",
    "  masked_encoding = tf.keras.layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same')(masked_encoding)\n",
    "  masked_encoding = tf.keras.layers.BatchNormalization()(masked_encoding)\n",
    "  masked_encoding = tf.keras.layers.LeakyReLU()(masked_encoding)\n",
    "  # 7x7x128\n",
    "\n",
    "  reference_image = tf.keras.Input(shape=(28,28,1,), name='reference_image')\n",
    "  \n",
    "  reference_encoding = tf.keras.layers.Conv2D(64, (5, 5),\n",
    "                                                  strides=(1, 1), \n",
    "                                                  padding='same', \n",
    "                                                  input_shape=(28, 28, 1))(reference_image)\n",
    "  reference_encoding = tf.keras.layers.BatchNormalization()(reference_encoding)\n",
    "  reference_encoding = tf.keras.layers.LeakyReLU()(reference_encoding)\n",
    "  # 28x28x64\n",
    "    \n",
    "  reference_encoding = tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')(reference_encoding)\n",
    "  reference_encoding = tf.keras.layers.BatchNormalization()(reference_encoding)\n",
    "  reference_encoding = tf.keras.layers.LeakyReLU()(reference_encoding)\n",
    "  # 14x14x64\n",
    "\n",
    "  reference_encoding = tf.keras.layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same')(reference_encoding)\n",
    "  reference_encoding = tf.keras.layers.BatchNormalization()(reference_encoding)\n",
    "  reference_encoding = tf.keras.layers.LeakyReLU()(reference_encoding)\n",
    "  # 7x7x128\n",
    "\n",
    "\n",
    "  encoding = tf.keras.layers.concatenate([masked_encoding, reference_encoding], axis=-1)\n",
    "  \n",
    "  # Decoder\n",
    "  encoding = tf.keras.layers.Conv2DTranspose(128, (5, 5), \n",
    "                                              strides=(1, 1), \n",
    "                                              padding='same', \n",
    "                                              use_bias=False, \n",
    "                                              input_shape=(7, 7, 256))(encoding)\n",
    "  encoding = tf.keras.layers.BatchNormalization()(encoding)\n",
    "  encoding = tf.keras.layers.LeakyReLU()(encoding)\n",
    "  # 7x7x128\n",
    "\n",
    "  encoding = tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)(encoding)\n",
    "  encoding = tf.keras.layers.BatchNormalization()(encoding)\n",
    "  encoding = tf.keras.layers.LeakyReLU()(encoding)\n",
    "  # 14x14x64\n",
    "\n",
    "  generated_patch = tf.keras.layers.Conv2DTranspose(1, (5, 5), \n",
    "                                                    strides=(2, 2), \n",
    "                                                    padding='same', \n",
    "                                                    use_bias=False, \n",
    "                                                    activation='tanh')(encoding)\n",
    "  # 28x28x1\n",
    "  \n",
    "  return tf.keras.Model(inputs=[masked_image, reference_image], outputs=generated_patch)\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0IKnaCtg6WE"
   },
   "source": [
    "### Discriminador\n",
    "\n",
    "El discriminador recibe como entrada dos imágenes: una imágen que puede ser real o tener una sección regenerada por el generador, y otra imágen de referencia de ese mismo dígito. Su objetivo ees distinguir entre imágenes reales e imágenes regeneradas por el generador, basandose tanto en la primer imagen como en la imagen de referencia.\n",
    "\n",
    "La arquitectura del discriminador consiste de dos Encoders y un clasificador. Uno de los encoders recibe la primer imágen (la cual puede ser real o generada), y el otro Encoder recibe la imágen de referencia. Cada enconder consiste de 2 capas convolucionales, con función de activación Leaky ReLU y Dropout.\n",
    "\n",
    "Luego, los resultados de ambos encoders son concatenados y enviados al clasificador. El clasificador aplica 2 capas fully-connected para determinar si la primer imágen es real o falsa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "  image = tf.keras.Input(shape=(28,28,1,), name='image')\n",
    "  \n",
    "  # Image encoder\n",
    "  image_encoding = tf.keras.layers.Conv2D(64, (5, 5), \n",
    "                                               strides=(2, 2), \n",
    "                                               padding='same', \n",
    "                                               input_shape=(28, 28, 1),\n",
    "                                               activation=tf.nn.leaky_relu)(image)\n",
    "  image_encoding = tf.keras.layers.Dropout(0.3)(image_encoding)\n",
    "  # 14x14x64\n",
    "\n",
    "  image_encoding = tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same', activation=tf.nn.leaky_relu)(image_encoding)\n",
    "  image_encoding = tf.keras.layers.Dropout(0.3)(image_encoding)\n",
    "  # 7x7x128\n",
    "\n",
    "  image_encoding = tf.keras.layers.Flatten()(image_encoding)\n",
    "\n",
    "\n",
    "  reference = tf.keras.Input(shape=(28,28,1,), name='reference')\n",
    "  \n",
    "  # Reference encoder\n",
    "  reference_encoding = tf.keras.layers.Conv2D(64, (5, 5), \n",
    "                                               strides=(2, 2), \n",
    "                                               padding='same', \n",
    "                                               input_shape=(28, 28, 1),\n",
    "                                               activation=tf.nn.leaky_relu)(reference)\n",
    "  reference_encoding = tf.keras.layers.Dropout(0.3)(reference_encoding)\n",
    "  # 14x14x64\n",
    "\n",
    "  reference_encoding = tf.keras.layers.Conv2D(128, (5, 5), \n",
    "                                              strides=(2, 2), \n",
    "                                              padding='same', \n",
    "                                              activation=tf.nn.leaky_relu)(reference_encoding)\n",
    "  reference_encoding = tf.keras.layers.Dropout(0.3)(reference_encoding)\n",
    "  # 7x7x128\n",
    "\n",
    "  reference_encoding = tf.keras.layers.Flatten()(reference_encoding)\n",
    "\n",
    "  encoding = tf.keras.layers.concatenate([image_encoding, reference_encoding], axis=1)\n",
    "\n",
    "  # Classifier\n",
    "  encoding = tf.keras.layers.Dense(1024, activation=tf.nn.leaky_relu)(encoding)\n",
    "  logits = tf.keras.layers.Dense(1)(encoding)\n",
    "  \n",
    "  return tf.keras.Model(inputs=[image, reference], outputs=logits)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gDkA05NE6QMs"
   },
   "outputs": [],
   "source": [
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "masked_image (InputLayer)       (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reference_image (InputLayer)    (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 28, 28, 64)   1664        masked_image[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 28, 28, 64)   1664        reference_image[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 28, 28, 64)   256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 28, 28, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 28, 28, 64)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 28, 28, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 14, 14, 64)   102464      leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 14, 14, 64)   102464      leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 14, 14, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 14, 14, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 14, 14, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 14, 14, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 7, 7, 128)    73856       leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 7, 7, 128)    73856       leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 7, 7, 128)    512         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 7, 7, 128)    512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 7, 7, 128)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 7, 7, 128)    0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 7, 7, 256)    0           leaky_re_lu_2[0][0]              \n",
      "                                                                 leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 7, 7, 128)    819200      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 7, 7, 128)    512         conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 7, 7, 128)    0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 14, 14, 64)   204800      leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 14, 14, 64)   256         conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 14, 14, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 28, 28, 1)    1600        leaky_re_lu_7[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,384,384\n",
      "Trainable params: 1,382,976\n",
      "Non-trainable params: 1,408\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image (InputLayer)              (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reference (InputLayer)          (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 14, 14, 64)   1664        image[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 14, 14, 64)   1664        reference[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 14, 14, 64)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 14, 14, 64)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 7, 7, 128)    204928      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 7, 7, 128)    204928      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 7, 7, 128)    0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 7, 7, 128)    0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 6272)         0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 6272)         0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 12544)        0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         12846080    concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            1025        dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 13,260,289\n",
      "Trainable params: 13,260,289\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FMYgY_mPfTi"
   },
   "source": [
    "## Funciones de costo y optimizadores\n",
    "\n",
    "A continuación se definen las funciones de costo para ambos modelos, y los optimizadores a utilizar para minimizar dichas funciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jd-3GCUEiKtv"
   },
   "source": [
    "### Función de costo del generador\n",
    "La función de costo del generador consiste de una función de Sigmoid Cross Entropy sobre las imágenes generadas. Se busca que la salida del discriminador sea 1 (reales) para dichas imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90BIcCKcDMxz"
   },
   "outputs": [],
   "source": [
    "def generator_loss(generated_output):\n",
    "    return tf.losses.sigmoid_cross_entropy(tf.ones_like(generated_output), generated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PKY_iPSPNWoj"
   },
   "source": [
    "### Función de costo del discriminador\n",
    "\n",
    "La función de costo del discriminador tiene dos partes:\n",
    "1. Costo sobre imágenes reales: consiste de una Sigmoid Cross Entropy sobre las imágenes reales. Se busca que la salida del discriminador sea 1 (reales) para dichas imágenes.\n",
    "2. Costo sobre imágenes generadas: consiste de una Sigmoid Cross Entropy sobre las imágenes generadas por el generador. Se busca que la salida del discriminador sea 0 (falsas) para dichas imágenes.\n",
    "\n",
    "Luego se suman estas dos funciones para obtener la función de costo del discriminador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkMNfBWlT-PV"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, generated_output):\n",
    "    real_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=tf.ones_like(real_output), logits=real_output)\n",
    "    generated_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=tf.zeros_like(generated_output), logits=generated_output)\n",
    "    total_loss = real_loss + generated_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MgIc7i0th_Iu"
   },
   "source": [
    "### Optimizadores\n",
    "Se utiliza un optimizador Adam para cada modelo, ya que estos se entrenan en paralelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iWCn_PVdEJZ7"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "discriminator_optimizer = tf.train.AdamOptimizer(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mWtinsGDPJlV"
   },
   "source": [
    "## Checkpoints\n",
    "Los parámetros de ambos modelos se almacenarán como checkpoints durante el entrenamiento, para poder recuperarlos luego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CA1w-7s2POEy"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = os.path.join(EXPERIMENT_DIR, 'train/')\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rw1fkAczTQYh"
   },
   "source": [
    "## Configuración del entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jylSonrqSWfi"
   },
   "source": [
    "### Paso de entrenamiento\n",
    "\n",
    "En cada paso del entrenamiento, el generador recibe un batch de imágenes enmascaradas junto con sus respectivas imágenes de referencias, y regenera las secciones faltantes. Estas regiones generadas son luego insertadas en las imágenes del batch, completando las secciones faltantes con las regiones generadas por el generador. Luego, el discriminador recibe por un lado estas imágenes con las secciones regeneradas junto con sus imágenes de referencia, y por otro lado un batch de imágenes reales, también con sus imágenes de referencia. \n",
    "\n",
    "A continuación, en base a los resultados del discriminador se calculan las funciones de costo de ambos modelos. Y finalmente, se aplica un paso de los optimizadores para cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_image(gen_image, image):\n",
    "  \"\"\"\n",
    "  Apply the given patch to the image.\n",
    "  The patch is applied at the center of the image, assuming a 7x7 patch and a 28x28 image.\n",
    "  \"\"\"\n",
    "  patch_start = (IMAGE_SIZE - PATCH_SIZE) // 2\n",
    "  patch_end = patch_start + PATCH_SIZE\n",
    "\n",
    "  patch = gen_image[:, patch_start:patch_end, patch_start:patch_end, :]\n",
    "  \n",
    "  # TODO: See if this could be done more efficiently.\n",
    "\n",
    "  upper_edge = image[:, :patch_start, :, :]\n",
    "  lower_edge = image[:, patch_end:, :, :]\n",
    "\n",
    "  middle_left = image[:, patch_start:patch_end, :patch_start, :]\n",
    "  middle_right = image[:, patch_start:patch_end, patch_end:, :]\n",
    "\n",
    "  middle = tf.concat([middle_left, patch, middle_right], axis=2)\n",
    "  return tf.concat([upper_edge, middle, lower_edge], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3t5ibNo05jCB"
   },
   "outputs": [],
   "source": [
    "def train_step(full_images, full_reference_images, masked_images, masked_reference_images):\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    \n",
    "    generated_patches = generator([masked_images, masked_reference_images], training=True)\n",
    "    generated_images = patch_image(generated_patches, masked_images)\n",
    "    \n",
    "    real_output = discriminator([full_images, full_reference_images], training=True)\n",
    "    generated_output = discriminator([generated_images, masked_reference_images], training=True)\n",
    "    \n",
    "    gen_loss = generator_loss(generated_output)\n",
    "    disc_loss = discriminator_loss(real_output, generated_output)\n",
    "\n",
    "  gradients_of_generator = gen_tape.gradient(gen_loss, generator.variables)\n",
    "  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.variables)\n",
    "\n",
    "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.variables))\n",
    "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.variables))\n",
    "  \n",
    "  return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6TSZgwc2BUQ-"
   },
   "source": [
    "Debido a que ejecutar el entrenamiento de forma secuencial suele ser más lento que ejecutar el grafo de operaciones equivalente, utilizamos la función [tf.contrib.eager.defun](https://www.tensorflow.org/api_docs/python/tf/contrib/eager/defun) para generar el grafo de operaciones y así obtener una mejora en la performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iwya07_j5p2A"
   },
   "outputs": [],
   "source": [
    "train_step = tf.contrib.eager.defun(train_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceso completo de entrenamiento\n",
    "Aquí definimos el proceso completo de entrenamiento. Se itera sobre todo el dataset por cada epoch, y por cada batch del dataset se aplica un paso de entrenamiento. Luego de cada epoch, se muestran las imágenes de validación con las secciones regeneradas por el discriminador, y se grafican las funciones de costo de ambos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2M7LmLtGEMQJ"
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  gen_losses = []\n",
    "  disc_losses = []\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    for images in dataset:\n",
    "      (full_images, full_reference_images) = images[0]\n",
    "      (masked_images, unmasked_images, masked_reference_images) = images[1]\n",
    "      gen_loss, disc_loss = train_step(full_images, full_reference_images, masked_images, masked_reference_images)\n",
    "    \n",
    "    # Only store losses after each epoch\n",
    "    gen_losses.append(gen_loss)\n",
    "    disc_losses.append(disc_loss)\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             validation_masked_images,\n",
    "                             validation_references)\n",
    "    plot_losses(gen_losses, disc_losses)\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print ('Time taken for epoch {} is {} sec'.format(epoch + 1,\n",
    "                                                      time.time()-start))\n",
    "  # generating after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           validation_masked_images,\n",
    "                           validation_references)\n",
    "  plot_losses(gen_losses, disc_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(gen_loss, disc_loss): \n",
    "  plt.plot(range(1, len(gen_loss) + 1), gen_loss)\n",
    "  plt.plot(range(1, len(disc_loss) + 1), disc_loss)\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.title('Generator and Discriminator Losses')\n",
    "  plt.legend(['Gen loss', 'Disc loss'])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RmdVsmvhPxyy"
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(generator, epoch, masked_images, reference_images):\n",
    "  # make sure the training parameter is set to False because we\n",
    "  # don't want to train the batchnorm layer when doing inference.\n",
    "  patches = generator([masked_images, reference_images], training=False)\n",
    "  generated_images = patch_image(patches, masked_images)\n",
    "  \n",
    "  show_and_save_images(generated_images, os.path.join(VALIDATION_IMGS_DIR, 'image_at_epoch_{:04d}.png'.format(epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dZrd4CdjR-Fp"
   },
   "source": [
    "## Entrenamiento de la GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ly3UN0SLLY2l",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "No algorithm worked! [Op:Conv2D]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-9f989f247f6d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0;34m(\u001b[0m\u001b[0mfull_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_reference_images\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0;34m(\u001b[0m\u001b[0mmasked_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munmasked_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_reference_images\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0mgen_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_reference_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_reference_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Only store losses after each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-45533991721f>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(full_images, full_reference_images, masked_images, masked_reference_images)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgen_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdisc_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mgenerated_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmasked_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_reference_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mgenerated_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_patches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m           raise ValueError('A layer\\'s `call` method should return a Tensor '\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    843\u001b[0m     outputs, _ = self._run_internal_graph(inputs,\n\u001b[1;32m    844\u001b[0m                                           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m                                           mask=masks)\n\u001b[0m\u001b[1;32m    846\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m   1029\u001b[0m                     computed_tensor, **kwargs)\n\u001b[1;32m   1030\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m                 \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'compute_mask'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m                   output_masks = layer.compute_mask(computed_tensor,\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         name=self.name)\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    980\u001b[0m           \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m           name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_eager_fallback\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name, ctx)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \"dilations\", dilations)\n\u001b[1;32m   1021\u001b[0m   _result = _execute.execute(b\"Conv2D\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0;32m-> 1022\u001b[0;31m                              ctx=_ctx, name=name)\n\u001b[0m\u001b[1;32m   1023\u001b[0m   _execute.record_gradient(\n\u001b[1;32m   1024\u001b[0m       \"Conv2D\", _inputs_flat, _attrs, _result, name)\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: No algorithm worked! [Op:Conv2D]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rfM4YcPVPkNO"
   },
   "source": [
    "**Restauración del último checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XhXsd0srPo8c"
   },
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4M_vIbUi7c0"
   },
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mLskt7EfXAjr"
   },
   "source": [
    "Luego del entrenamiento, visualizamos los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WfO5wCdclHGL"
   },
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(name):\n",
    "  return PIL.Image.open(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imágenes enmascaradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image('validation_imgs/validation_masked_images.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imágenes de referencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image('validation_imgs/validation_reference_images.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imágenes generadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image('validation_imgs/image_at_epoch_{:04d}.png'.format(EPOCHS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imágenes reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image('validation_imgs/validation_unmasked_images.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NywiH3nL8guF"
   },
   "source": [
    "### GIF del proceso de entrenamiento\n",
    "\n",
    "A continuación generamos un gif mostrando la evolución del generador sobre las imágenes de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IGKQgENQ8lEI"
   },
   "outputs": [],
   "source": [
    "with imageio.get_writer('mnist-inpainting.gif', mode='I') as writer:\n",
    "  filenames = glob.glob('image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  last = -1\n",
    "  for i,filename in enumerate(filenames):\n",
    "    frame = 2*(i**0.5)\n",
    "    if round(frame) > round(last):\n",
    "      last = frame\n",
    "    else:\n",
    "      continue\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)\n",
    "    \n",
    "# this is required in order to display the gif inside the notebook\n",
    "os.system('cp mnist-inpainting.gif mnist-inpainting.gif.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uV0yiKpzNP1b"
   },
   "outputs": [],
   "source": [
    "display.Image(filename=\"mnist-inpainting.gif.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images):\n",
    "  fig = plt.figure(figsize=(8,8))\n",
    "  for i in range(images.shape[0]):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(images[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "    plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(generator, masked_images, reference_images):\n",
    "  # make sure the training parameter is set to False because we\n",
    "  # don't want to train the batchnorm layer when doing inference.\n",
    "  patches = generator([masked_images, reference_images], training=False)\n",
    "  generated_images = patch_image(patches, masked_images)\n",
    "  return generated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_show_images(generator, masked_images, reference_images):\n",
    "  generated_images = generate_images(generator, masked_images, reference_images)\n",
    "  show_images(generated_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_show_images(generator,\n",
    "                validation_masked_images,\n",
    "                validation_references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_images(validation_references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación con referencia incorrecta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict of at least\n",
    "test_dict = {}\n",
    "for i in range(10):\n",
    "  test_dict[i] = []\n",
    "for image, label in zip(test_images, test_labels):\n",
    "  if all(len(imgs) >=  for imgs in test_dict.values()):\n",
    "      break\n",
    "  test_dict[label].append(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = []\n",
    "masked = []\n",
    "references = []\n",
    "for imgs in test_dict.values():\n",
    "  to_mask = imgs[0]\n",
    "  reference = imgs[1]\n",
    "  real.append(to_mask)\n",
    "  masked.append(mask(to_mask, reference)[0])\n",
    "  references.append(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "for i, real_img in enumerate(real):\n",
    "  plt.subplot(3, 10, i+1)\n",
    "  plt.imshow(real_img[:,:,0], cmap='gray')\n",
    "  plt.axis('off')\n",
    "for i, masked_img in enumerate(masked):\n",
    "  plt.subplot(3, 10, i+11)\n",
    "  plt.imshow(masked_img[:,:,0], cmap='gray')\n",
    "  plt.axis('off')\n",
    "for i, reference in enumerate(references):\n",
    "  plt.subplot(3, 10, i+21)\n",
    "  plt.imshow(reference[:,:,0], cmap='gray')\n",
    "  plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer fila: Reales\n",
    "Segunda fila: Enmascaradas\n",
    "Tercer fila: Referencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = []\n",
    "for masked_img in masked:\n",
    "  generated_images.append([])\n",
    "  for reference in references:\n",
    "    generated = generate_images(generator, np.expand_dims(masked_img, axis=0), np.expand_dims(reference, axis=0))\n",
    "    generated_images[-1].append(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "for i, ref in enumerate(references):\n",
    "  plt.subplot(11, 11, i + 2)\n",
    "  plt.imshow(references[i][:,:,0] * 127.5 + 127.5, cmap='gray')\n",
    "  plt.axis('off')\n",
    "\n",
    "for i in range(len(generated_images)):\n",
    "\n",
    "  plt.subplot(11, 11, (11*(i+1)) + 1)\n",
    "  plt.imshow(masked[i][:,:,0] * 127.5 + 127.5, cmap='gray')\n",
    "  plt.axis('off')\n",
    "  \n",
    "  for j in range(len(generated_images[i])):\n",
    "    plt.subplot(11, 11, (11*(i+1)) + j + 2)\n",
    "    plt.imshow(generated_images[i][j][0,:,:,0] * 127.5 + 127.5, cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matriz de imágenes generadas, usando la imágen de referencia de la columna correspondiente."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dcgan.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
