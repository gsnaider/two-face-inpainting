{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0TD5ZrvEMbhZ"
   },
   "source": [
    "# MNIST Inpainting con imagen de referencia\n",
    "En este experimento se implementará y entrenará un modelo de image inpainting sobre el dataset de MNIST. Se entrenará un modelo capaz de regenerar secciones faltantes dentro de imágenes de MNIST de forma realista. El modelo consiste de Deep Convolutional Networks, y se entrenará mediante un framework GAN, es decir, se entrenará un modelo generador y un discriminador en simultaneo.\n",
    "\n",
    "En esta primera versión, el generador solo recibe la imágen con la sección a ser regenerada. En otro experimento se buscará pasar una segunda imágen de referencia al modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "## Imports y setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio in /home/gaston/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages (2.4.1)\r\n",
      "Requirement already satisfied: pillow in /home/gaston/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages (from imageio) (5.3.0)\r\n",
      "Requirement already satisfied: numpy in /home/gaston/.local/lib/python3.5/site-packages (from imageio) (1.14.3)\r\n"
     ]
    }
   ],
   "source": [
    "# to generate gifs\n",
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YfIk2es3hJEd"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import time\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iYn4MdZnKCey"
   },
   "source": [
    "## Carga del dataset\n",
    "\n",
    "Usaremos el dataset de MNIST para entrenar al generador y al discriminador, el cual viene incluido dentro de la API de tf.keras. El dataset es dividido en dos para el entrenamiento: una mitad se utilizará como imágenes reales para entrenar al discriminador, y a la otra mitad se le aplicará una mascara para ocultar una región, la cual deberá ser regenerada por el generador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(image, reference_image):\n",
    "  \"\"\"\n",
    "  Applies a mask of zeroes of size 7x7 at the center of the image.\n",
    "  Returns a tuple of the masked image and the original image.\n",
    "  \"\"\"\n",
    "  upper_edge = tf.ones([10,28,1], tf.float32)\n",
    "  lower_edge = tf.ones([11,28,1], tf.float32)\n",
    "  \n",
    "  middle_left = tf.ones([7,10,1], tf.float32)\n",
    "  middle_right = tf.ones([7,11,1], tf.float32)\n",
    "  \n",
    "  zeros = tf.zeros([7,7,1], tf.float32)\n",
    "  \n",
    "  middle = tf.concat([middle_left, zeros, middle_right], axis=1)\n",
    "  mask = tf.concat([upper_edge, middle, lower_edge], axis=0)\n",
    "  \n",
    "  return (image * mask, image, reference_image)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reference_dictionary(reference_labels):\n",
    "  reference_dict = {}\n",
    "  for label in range(10):\n",
    "    reference_dict[label] = np.where(train_reference_labels == label)[0]\n",
    "  return reference_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_references(reference_pool, labels, reference_dict):\n",
    "  reference_images = []\n",
    "  for label in labels:\n",
    "    reference_idx = np.random.choice(reference_dict[label])\n",
    "    reference_images.append(reference_pool[reference_idx])\n",
    "  return np.array(reference_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFC2ghIdiZYE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "# Normalize the images to [-1, 1]\n",
    "train_images = (train_images - 127.5) / 127.5\n",
    "test_images = (test_images - 127.5) / 127.5\n",
    "\n",
    "indeces = np.random.permutation(len(train_images))\n",
    "train_images = train_images[indeces]\n",
    "train_labels = train_labels[indeces]\n",
    "\n",
    "# Split between training and validation sets\n",
    "TRAINING_SAMPLES = int(len(train_images) * 0.9)\n",
    "train_set_images = train_images[:TRAINING_SAMPLES,:,:,:]\n",
    "validation_set_images = train_images[TRAINING_SAMPLES:,:,:,:]\n",
    "\n",
    "# TODO: generar diccionario de referencias para validation.\n",
    "\n",
    "train_set_labels = train_labels[:TRAINING_SAMPLES]\n",
    "validation_set_labels = train_labels[TRAINING_SAMPLES:]\n",
    "\n",
    "# Split training images between masked and full images.\n",
    "REAL_IMAGES_BOUNDRY = int(len(train_set_images) * 0.4)\n",
    "MASKED_IMAGES_BOUNDRY = REAL_IMAGES_BOUNDRY + int(len(train_set_images) * 0.4)\n",
    "\n",
    "train_full_images = train_set_images[:REAL_IMAGES_BOUNDRY,:,:,:]\n",
    "train_masked_images = train_set_images[REAL_IMAGES_BOUNDRY:MASKED_IMAGES_BOUNDRY,:,:,:]\n",
    "train_reference_images = train_set_images[MASKED_IMAGES_BOUNDRY:,:,:,:]\n",
    "\n",
    "train_full_labels = train_set_labels[:REAL_IMAGES_BOUNDRY]\n",
    "train_masked_labels = train_set_labels[REAL_IMAGES_BOUNDRY:MASKED_IMAGES_BOUNDRY]\n",
    "train_reference_labels = train_set_labels[MASKED_IMAGES_BOUNDRY:]\n",
    "\n",
    "\n",
    "train_reference_dict = create_reference_dictionary(train_reference_labels)\n",
    "train_full_references = get_references(train_reference_images, train_full_labels, train_reference_dict)\n",
    "train_masked_references = get_references(train_reference_images, train_masked_labels, train_reference_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABItJREFUeJzt3UFy6jgUQNHQ1fvKZ2XAyoCVuUc9jJSKvw3xPWfqgFMktzR4SD4ty/IB9Pzz6l8AeA3xQ5T4IUr8ECV+iBI/RIkfosQPUeKHqH/3vNnpdPJ1QtjYsiyn7/yclR+ixA9R4oco8UOU+CFK/BAlfogSP0SJH6LED1HihyjxQ5T4IUr8ECV+iBI/RIkfosQPUeKHKPFDlPghSvwQJX6IEj9EiR+ixA9R4oco8UOU+CFK/BC16yO6+Zk/f/6sur7G5XJZ9frH4/HltefzOXzt9XpddW/GrPwQJX6IEj9EiR+ixA9R4oco8UPUaVmW/W52Ou13s1/kfr8Pr285x39no+8IfHx8fJzP531+kV9mWZbTd37Oyg9R4oco8UOU+CFK/BAlfogSP0TZz7+D2b70Lef4s1n5bE/9WmvOA5h9LrPvR/gewJiVH6LED1HihyjxQ5T4IUr8EGVL7w7Wfsazcd3tdvvxa19p6/+90+lbO1sPx5ZeYEj8ECV+iBI/RIkfosQPUeKHKHP+Haz9jI86rzbn34Y5PzAkfogSP0SJH6LED1HihyjxQ5Sju/m13vmsgt/Ayg9R4oco8UOU+CFK/BAlfogSP0SZ8+9gtq98y0d0H9nWjxc/Ois/RIkfosQPUeKHKPFDlPghSvwQ5dx+NnW/37+8tvb7DdVz+Wec2w8MiR+ixA9R4oco8UOU+CHKll5WmY3r1ozzbrfbj1/LnJUfosQPUeKHKPFDlPghSvwQJX6IsqWXodmcfrRldy1bdn/Gll5gSPwQJX6IEj9EiR+ixA9R4oco+/njXjnHP5/Pm703c1Z+iBI/RIkfosQPUeKHKPFDlPghypw/7nK5bPr+o1n+4/HY9N6MWfkhSvwQJX6IEj9EiR+ixA9R4ococ/6Dm+3Hn+3nn7ndbsPrZvnvy8oPUeKHKPFDlPghSvwQJX6I8ojuAxiN89aO8majOsdvvx+P6AaGxA9R4oco8UOU+CFK/BAlfoiypfcNvPIx2eb4XVZ+iBI/RIkfosQPUeKHKPFDlPghypz/DWz5mOzZ0drX63Wze8/ef+t7M2blhyjxQ5T4IUr8ECV+iBI/RIkfopzbv4OtH5NdNTuL4Pl8fnntyN8xcG4/MCR+iBI/RIkfosQPUeKHKFt6/4LZqM4obxtbfu5HHgX+z8oPUeKHKPFDlPghSvwQJX6IEj9EmfMfwOh47tm219n1Lc1m6Z+fn8Pra+b4s+PS3/lz+1us/BAlfogSP0SJH6LED1HihyjxQ5Sju3ewdj//EWbKr7Dmf/t8Pg+vv/PfxNHdwJD4IUr8ECV+iBI/RIkfosQPUeb8cDDm/MCQ+CFK/BAlfogSP0SJH6LED1HihyjxQ5T4IUr8ECV+iBI/RIkfosQPUeKHKPFDlPghSvwQJX6IEj9EiR+ixA9R4oco8UOU+CFK/BAlfogSP0SJH6J2fUQ38D6s/BAlfogSP0SJH6LED1HihyjxQ5T4IUr8ECV+iBI/RIkfosQPUeKHKPFDlPghSvwQJX6IEj9EiR+ixA9R4oco8UPUf1xizi39yiE3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABm1JREFUeJzt3U+IjX0fx/E5T7eFGkzJwk4ppdiIKaUoZKcpSlZiI8nIkoWdYmMja1lRrFjJn+WUstPIn6aIktIIpZjhuld3PU895ztzn3Nm5sz5vF7b71zX71rMu9/id65zWk3TDAF5/rPUDwAsDfFDKPFDKPFDKPFDKPFDKPFDKPFDKPFDqL8Wc7FWq+XjhLDAmqZpzefv7PwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQalHf56f/bNq0qZw/ffq0nM/MzJTzHTt2tJ29e/euvJaFZeeHUOKHUOKHUOKHUOKHUOKHUI76wu3Zs6ecr1mzpqv7r1u3ru3MUd/SsvNDKPFDKPFDKPFDKPFDKPFDKPFDKOf8A27lypXl/Ny5cwu6/uHDh9vOnj17tqBrU7PzQyjxQyjxQyjxQyjxQyjxQyjxQ6hW0zSLt1irtXiLMTQ0NDQ0MjJSzqenp7u6/5cvX8r56Oho29nU1FRXa/P/NU3Tms/f2fkhlPghlPghlPghlPghlPghlPghlPf5B9zp06cX9P63b98u587y+5edH0KJH0KJH0KJH0KJH0KJH0I56hsA1c9gj4+Pd3XvuV75vnfvXlf3Z+nY+SGU+CGU+CGU+CGU+CGU+CGU+CGUc/4BsH379raz6jMA8zE7O1vOHzx40NX9WTp2fgglfgglfgglfgglfgglfgglfgjlnH8A7Nu3b8Hu/ejRowW7N0vLzg+hxA+hxA+hxA+hxA+hxA+hxA+hWnN9L3tPF2u1Fm+xAbJ+/fpy/urVq7az4eHhrtbetWtXOZ+YmOjq/vRe0zSt+fydnR9CiR9CiR9CiR9CiR9CiR9CiR9CeZ9/GTh16lQ57+Ys//Xr1+X8xYsX5XzDhg3l/OTJk21nc/2mwNTUVDl/+PBhOX/z5k3b2devX8trE9j5IZT4IZT4IZT4IZT4IZT4IZRXepeBO3fulPNDhw51fO+zZ8+W80+fPpXz69evl/O1a9f+62fqleprx8fGxsprf/z40evHWTRe6QVK4odQ4odQ4odQ4odQ4odQ4odQzvn7wOrVq8v527dvy/nIyEjHaz9//rycb926teN797Nr166V87k+/9DPnPMDJfFDKPFDKPFDKPFDKPFDKPFDKOf8feD8+fPl/NKlS4v0JIvr27dv5XxycrKc79y5s+O1v3//Xs63bNlSzt+/f9/x2gvNOT9QEj+EEj+EEj+EEj+EEj+EEj+E8hPdfWDv3r1L/Qgde/nyZTk/ePBg29lcvwnw8+fPcn7//v1yvn///razVatWldeOjo6W834+558vOz+EEj+EEj+EEj+EEj+EEj+EEj+Ecs5P6fHjx+X8+PHj5fzDhw+9fJz/MT093fG1v379KucfP37s+N7LhZ0fQokfQokfQokfQokfQokfQjnqGwCtVvtvav7z50957dWrV8v5hQsXyvnMzEw5rwwPD5fzo0ePlvOxsbGO137y5Ek5n5iY6Pjey4WdH0KJH0KJH0KJH0KJH0KJH0KJH0I55x8A1c+sz87OltdOTU2V8927d3f0TP/YvHlz29n4+Hh57caNG7ta+/Pnz21nFy9e7Oreg8DOD6HED6HED6HED6HED6HED6HED6Fa1RlxzxdrtRZvsWXk2LFj5fzGjRuL9CTLy1z/u0eOHGk7u3v3bq8fp280TdP+Cx7+i50fQokfQokfQokfQokfQokfQokfQnmfvw/cvHmznG/btq2cnzlzppePs2xcvny5nA/yWX4v2PkhlPghlPghlPghlPghlPghlFd6l4EVK1aU8wMHDrSdzfUz1idOnOjomXrhypUr5fzWrVvlfHJyspz//v37Xz/TIPBKL1ASP4QSP4QSP4QSP4QSP4QSP4Ryzg8Dxjk/UBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hGo1TbPUzwAsATs/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hPobZ1IN2iC2sg8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABt5JREFUeJzt3U+IjX0fx3GDDYnSYCPTsJBMsRhpJOxMDMLKXnYsJaVosLxTpvwpGwtbGzVhIRYWWEyJLBCymc0Uoca/86yep57F+R73jDnz5/N6bT+uOVd33ve1+DnXdDQajXlAnvnTfQPA9BA/hBI/hBI/hBI/hBI/hBI/hBI/hBI/hFrYzg/r6OjwzwlhijUajY4/+XOe/BBK/BBK/BBK/BBK/BBK/BBK/BBK/BBK/BBK/BBK/BBK/BBK/BBK/BBK/BCqrd/nZ+5ZuLD+K3T79u2mW39/f3ltX19fuT979qzcqXnyQyjxQyjxQyjxQyjxQyjxQyhHfeFaHdWdOXOm3A8dOlTuPT09//qe/qu7u7vcHfVNjic/hBI/hBI/hBI/hBI/hBI/hBI/hHLOP8f19vaW+9DQULlv3br1b94OM4gnP4QSP4QSP4QSP4QSP4QSP4QSP4Ryzj8HbN68uek2PDxcXtvZ2Vnuo6Oj5X7q1KlyP3bsWNPt6dOn5bWt7p3J8eSHUOKHUOKHUOKHUOKHUOKHUOKHUB2NRqN9H9bR0b4Pm0NWrFhR7i9evJjwtR8/fiz3ixcvlvuVK1fKnfZrNBodf/LnPPkhlPghlPghlPghlPghlPghlK/0zgJXr14t9+o47+fPn+W1g4OD5X79+vVyZ/by5IdQ4odQ4odQ4odQ4odQ4odQ4odQzvlngCNHjpT7wMDAhH/25cuXy905fi5Pfgglfgglfgglfgglfgglfgglfgjl1d1tsGTJknIfGRkp93Xr1pX7o0ePmm579+4tr/3y5Uu5M/t4dTdQEj+EEj+EEj+EEj+EEj+EEj+E8n3+Nli0aFG5tzrHb+XChQtNN+f4NOPJD6HED6HED6HED6HED6HED6HED6Gc87fByZMnJ3X9jx8/yv3z58+T+vlk8uSHUOKHUOKHUOKHUOKHUOKHUF7d/RfMn1//P/T169fl3t3dXe4fPnwo966urnKfTtV/m4ULJ3fS3OoItJ1/t2cSr+4GSuKHUOKHUOKHUOKHUOKHUOKHUL7S+xfs3Lmz3Fud47cyOjo6qesrq1atKve+vr5yP3jwYLmvXLmy6dbf319e28rp06fL/dq1a023sbGxSX32XODJD6HED6HED6HED6HED6HED6HED6Gc8/8Fv379Kvffv3+Xe6v3Abx79+7f3tL/7Nq1q9wvXbpU7ps2bZrwZ0+1ixcvlvuePXuabgMDA+W1nz59mtA9zSae/BBK/BBK/BBK/BBK/BBK/BBK/BDKe/vb4M2bN+W+du3acm/13v6tW7c23e7fv19e29PTU+6tPH78uNyHh4ebbvv37y+v3bJly4Tu6U8MDQ2V+/Hjx6fss6ea9/YDJfFDKPFDKPFDKPFDKPFDKPFDKOf8bfD8+fNyb3XW3uq75WfPnm26/fPPP+W1HR31kfD58+fLfXBwsNy/f//edFu6dGl57YkTJ8r93Llz5V69J+H9+/fltevXry/38fHxcp9OzvmBkvghlPghlPghlPghlPghlFd3t0GrV0zfunWr3JctW1buX79+bbrduXOnvHbfvn3lvnHjxnKvjvJa+fz5c7m3Ombs6uoq96NHj0742m3btpX7gwcPyn028OSHUOKHUOKHUOKHUOKHUOKHUOKHUM752+Du3bvl/u3bt3JfvHhxuVevoW51Hn3gwIFyf/nyZblPxoIFC8q9u7u73FevXj3hzx4bGyv3t2/fTvhnzxae/BBK/BBK/BBK/BBK/BBK/BBK/BDKq7tngFav7n748GG5L1++/G/ezv/58uVLuT958qTcq19PvmHDhvLa7du3l/tk3Lt3r9x37949ZZ891by6GyiJH0KJH0KJH0KJH0KJH0KJH0I5558FVqxYUe43b95suvX29pbXdnZ2TuieZoJXr16Ve/U7C1r9eu9W/75hJnPOD5TED6HED6HED6HED6HED6Ec9c1xa9asKfcdO3aU++HDh8u91au/b9y40XRr9eu9qyPMefPmzRsZGSn38fHxcp+rHPUBJfFDKPFDKPFDKPFDKPFDKPFDKOf8MMc45wdK4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQ4odQHY1GY7rvAZgGnvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQSvwQ6j+EnTnlU4qakQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(4):\n",
    "  plt.imshow(train_masked_images[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "  plt.imshow(train_masked_references[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-yKCCQOoJ7cn"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = TRAINING_SAMPLES\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_full_images_ds = tf.data.Dataset.from_tensor_slices((train_full_images, train_full_references)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "train_masked_images_ds = tf.data.Dataset.from_tensor_slices((train_masked_images, train_masked_references)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).map(mask)\n",
    "\n",
    "train_dataset = tf.data.Dataset.zip((train_full_images_ds, train_masked_images_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_and_save_images(images, name):\n",
    "  fig = plt.figure(figsize=(4,4))\n",
    "  \n",
    "  for i in range(images.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(images[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "        \n",
    "  plt.savefig(name)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a subset of images for validation\n",
    "np.random.shuffle(validation_set_images)\n",
    "validation_images = validation_set_images[:16]\n",
    "\n",
    "validation_masked_images = []\n",
    "for image in validation_images:\n",
    "  masked_image, _ = mask(image)\n",
    "  validation_masked_images.append(masked_image.numpy())\n",
    "\n",
    "validation_masked_images = np.array(validation_masked_images)\n",
    "\n",
    "show_and_save_images(validation_masked_images, 'validation_masked_images.png')\n",
    "show_and_save_images(validation_images, 'validation_unmasked_images.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "## Creación de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-tEyxE-GMC48"
   },
   "source": [
    "### Generador\n",
    "\n",
    "El objetivo del generador es regenerar las secciones faltantes en las imágenes enmascaradas de forma realista. La arquitectura del generador consiste de un modelo Encoder-Decoder. El Encoder recibe la imágen con la región faltante, y aplica 3 capas convolucionales . El resultado del Encoder luego se pasa al Decoder. El Decoder aplica 3 capas convolucionales transpuestas hasta generar una imágen del tamaño de la región faltante. Esta imágen será el output del generator. Todas las capas convolucionales utilizan Batch Normalization y función de activación Leaky ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Generator, self).__init__()\n",
    "    \n",
    "    # Masked encoder\n",
    "    self.mask_enc_conv_1 = tf.keras.layers.Conv2D(64, (5, 5),\n",
    "                                                  strides=(1, 1), \n",
    "                                                  padding='same', \n",
    "                                                  input_shape=(28, 28, 1))\n",
    "    self.mask_enc_bn_1 = tf.keras.layers.BatchNormalization()\n",
    "    self.mask_enc_act_1 = tf.keras.layers.LeakyReLU()\n",
    "    # 28x28x64\n",
    "    \n",
    "    self.mask_enc_conv_2 = tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')\n",
    "    self.mask_enc_bn_2 = tf.keras.layers.BatchNormalization()\n",
    "    self.mask_enc_act_2 = tf.keras.layers.LeakyReLU()\n",
    "    # 14x14x64\n",
    "        \n",
    "    self.mask_enc_conv_3 = tf.keras.layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same')\n",
    "    self.mask_enc_bn_3 = tf.keras.layers.BatchNormalization()\n",
    "    self.mask_enc_act_3 = tf.keras.layers.LeakyReLU()\n",
    "    # 7x7x128\n",
    "\n",
    "    \n",
    "    # Reference encoder\n",
    "    self.ref_enc_conv_1 = tf.keras.layers.Conv2D(64, (5, 5), \n",
    "                                                 strides=(1, 1), \n",
    "                                                 padding='same', \n",
    "                                                 input_shape=(28, 28, 1))\n",
    "    self.ref_enc_bn_1 = tf.keras.layers.BatchNormalization()\n",
    "    self.ref_enc_act_1 = tf.keras.layers.LeakyReLU()\n",
    "    # 28x28x64\n",
    "    \n",
    "    self.ref_enc_conv_2 = tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')\n",
    "    self.ref_enc_bn_2 = tf.keras.layers.BatchNormalization()\n",
    "    self.ref_enc_act_2 = tf.keras.layers.LeakyReLU()\n",
    "    # 14x14x64\n",
    "        \n",
    "    self.ref_enc_conv_3 = tf.keras.layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same')\n",
    "    self.ref_enc_bn_3 = tf.keras.layers.BatchNormalization()\n",
    "    self.ref_enc_act_3 = tf.keras.layers.LeakyReLU()\n",
    "    # 7x7x128\n",
    "    \n",
    "    \n",
    "    # Decoder\n",
    "    self.dec_conv_1 = tf.keras.layers.Conv2DTranspose(128, (5, 5), \n",
    "                                                      strides=(1, 1), \n",
    "                                                      padding='same', \n",
    "                                                      use_bias=False, \n",
    "                                                      input_shape=(7, 7, 256))\n",
    "    self.dec_bn_1 = tf.keras.layers.BatchNormalization()\n",
    "    self.dec_act_1 = tf.keras.layers.LeakyReLU()\n",
    "    # 7x7x128\n",
    "\n",
    "    self.dec_conv_2 = tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False)\n",
    "    self.dec_bn_2 = tf.keras.layers.BatchNormalization()\n",
    "    self.dec_act_2 = tf.keras.layers.LeakyReLU()\n",
    "    # 7x7x64\n",
    "    \n",
    "    self.dec_conv_3 = tf.keras.layers.Conv2DTranspose(1, (5, 5), \n",
    "                                                      strides=(1, 1), \n",
    "                                                      padding='same', \n",
    "                                                      use_bias=False, \n",
    "                                                      activation='tanh')\n",
    "    # 7x7x1\n",
    "\n",
    "  def call(self, inputs, training=True):\n",
    "    \n",
    "    masked_image = inputs[0]\n",
    "    reference_image = inputs[1]\n",
    "    \n",
    "    # Apply masked image encoder\n",
    "    masked_encoding = self.mask_enc_conv_1(masked_image)\n",
    "    masked_encoding = self.mask_enc_bn_1(masked_encoding, training=training)\n",
    "    masked_encoding = self.mask_enc_act_1(masked_encoding)\n",
    "        \n",
    "    masked_encoding = self.mask_enc_conv_2(masked_encoding)\n",
    "    masked_encoding = self.mask_enc_bn_2(masked_encoding, training=training)\n",
    "    masked_encoding = self.mask_enc_act_2(masked_encoding)\n",
    "        \n",
    "    masked_encoding = self.mask_enc_conv_3(masked_encoding)\n",
    "    masked_encoding = self.mask_enc_bn_3(masked_encoding, training=training)\n",
    "    masked_encoding = self.mask_enc_act_3(masked_encoding)\n",
    "        \n",
    "    # Apply reference image encoder\n",
    "    reference_encoding = self.mask_enc_conv_1(reference_image)\n",
    "    reference_encoding = self.mask_enc_bn_1(reference_encoding, training=training)\n",
    "    reference_encoding = self.mask_enc_act_1(reference_encoding)\n",
    "        \n",
    "    reference_encoding = self.mask_enc_conv_2(reference_encoding)\n",
    "    reference_encoding = self.mask_enc_bn_2(reference_encoding, training=training)\n",
    "    reference_encoding = self.mask_enc_act_2(reference_encoding)\n",
    "        \n",
    "    reference_encoding = self.mask_enc_conv_3(reference_encoding)\n",
    "    reference_encoding = self.mask_enc_bn_3(reference_encoding, training=training)\n",
    "    reference_encoding = self.mask_enc_act_3(reference_encoding)\n",
    "    \n",
    "    # Join both the masked and reference encoding\n",
    "    encoding = tf.concat([masked_encoding, reference_encoding], axis=-1)\n",
    "    \n",
    "    # Apply the decoder to the joined encoding\n",
    "    encoding = self.dec_conv_1(encoding)\n",
    "    encoding = self.dec_bn_1(encoding, training=training)\n",
    "    encoding = self.dec_act_1(encoding)\n",
    "        \n",
    "    encoding = self.dec_conv_2(encoding)\n",
    "    encoding = self.dec_bn_2(encoding, training=training)\n",
    "    encoding = self.dec_act_2(encoding)\n",
    "    \n",
    "    generated_patch = self.dec_conv_3(encoding)\n",
    "    \n",
    "    return generated_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_encoder_model():\n",
    "  model = tf.keras.Sequential()\n",
    "\n",
    "  # Encoder\n",
    "  model.add(tf.keras.layers.Conv2D(64, (5, 5), strides=(1, 1), padding='same', input_shape=(28, 28, 1)))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.LeakyReLU())\n",
    "  assert model.output_shape == (None, 28, 28, 64)\n",
    "\n",
    "  model.add(tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.LeakyReLU())\n",
    "  assert model.output_shape == (None, 14, 14, 64)\n",
    "\n",
    "  model.add(tf.keras.layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.LeakyReLU())\n",
    "  assert model.output_shape == (None, 7, 7, 128)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6bpTcDqoLWjY"
   },
   "outputs": [],
   "source": [
    "def make_generator_decoder_model():\n",
    "  model = tf.keras.Sequential()\n",
    "  \n",
    "  # Decoder  \n",
    "  model.add(tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False, input_shape=(7, 7, 256)))\n",
    "  assert model.output_shape == (None, 7, 7, 128)  \n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "  model.add(tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "  assert model.output_shape == (None, 7, 7, 64)    \n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "  model.add(tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(1, 1), padding='same', use_bias=False, activation='tanh'))\n",
    "  assert model.output_shape == (None, 7, 7, 1)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0IKnaCtg6WE"
   },
   "source": [
    "### Discriminador\n",
    "\n",
    "El objetivo del discriminador es distinguir entre imágenes reales e imágenes regeneradas por el generador. El discriminador recibe como entrada una imágen de 28x28. Luego se aplican dos capas convolucionales sobre la imágen, y finalmente se aplica una capa fully-connected de una neurona para determinar si la imágen es real o falsa. Entre cada capa se utiliza Dropout y la función de activación Leaky ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Discriminator, self).__init__()\n",
    "    \n",
    "    # Image encoder\n",
    "    self.img_enc_conv_1 = tf.keras.layers.Conv2D(64, (5, 5), \n",
    "                                                 strides=(2, 2), \n",
    "                                                 padding='same', \n",
    "                                                 input_shape=(28, 28, 1),\n",
    "                                                 activation=tf.nn.leaky_relu)\n",
    "    self.img_enc_drop_1 = tf.keras.layers.Dropout(0.3)\n",
    "    # 14x14x64\n",
    "      \n",
    "    self.img_enc_conv_2 = tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same', activation=tf.nn.leaky_relu)\n",
    "    self.img_enc_drop_2 = tf.keras.layers.Dropout(0.3)\n",
    "    # 7x7x128\n",
    "     \n",
    "    self.img_enc_flat = tf.keras.layers.Flatten()\n",
    "    \n",
    "    \n",
    "    # Reference encoder\n",
    "    self.ref_enc_conv_1 = tf.keras.layers.Conv2D(64, (5, 5), \n",
    "                                                 strides=(2, 2), \n",
    "                                                 padding='same', \n",
    "                                                 input_shape=(28, 28, 1),\n",
    "                                                 activation=tf.nn.leaky_relu)\n",
    "    self.ref_enc_drop_1 = tf.keras.layers.Dropout(0.3)\n",
    "    # 14x14x64\n",
    "      \n",
    "    self.ref_enc_conv_2 = tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same', activation=tf.nn.leaky_relu)\n",
    "    self.ref_enc_drop_2 = tf.keras.layers.Dropout(0.3)\n",
    "    # 7x7x128\n",
    "     \n",
    "    self.ref_enc_flat = tf.keras.layers.Flatten()\n",
    "    \n",
    "    \n",
    "    # Classifier\n",
    "    self.class_fc_1 = tf.keras.layers.Dense(1024, activation=tf.nn.leaky_relu)\n",
    "    self.class_fc_2 = tf.keras.layers.Dense(1)\n",
    "        \n",
    "\n",
    "  def call(self, inputs, training=True):\n",
    "    \n",
    "    image = inputs[0]\n",
    "    reference = inputs[1]\n",
    "    \n",
    "    # Apply masked image encoder\n",
    "    img_encoding = self.img_enc_conv_1(image)\n",
    "    img_encoding = self.img_enc_drop_1(img_encoding)\n",
    "    \n",
    "    img_encoding = self.img_enc_conv_2(img_encoding)\n",
    "    img_encoding = self.img_enc_drop_2(img_encoding)\n",
    "    \n",
    "    img_encoding = self.img_enc_flat(img_encoding)\n",
    "        \n",
    "    # Apply reference image encoder\n",
    "    ref_encoding = self.ref_enc_conv_1(reference)\n",
    "    ref_encoding = self.ref_enc_drop_1(ref_encoding)\n",
    "    \n",
    "    ref_encoding = self.ref_enc_conv_2(ref_encoding)\n",
    "    ref_encoding = self.ref_enc_drop_2(ref_encoding)\n",
    "    \n",
    "    ref_encoding = self.ref_enc_flat(ref_encoding)\n",
    "        \n",
    "\n",
    "    # Join both the image and reference encoding\n",
    "    encoding = tf.concat([img_encoding, ref_encoding], axis=1)\n",
    "    \n",
    "    # Apply the classifier to the joined encoding\n",
    "    encoding = self.class_fc_1(encoding)\n",
    "    logits = self.class_fc_2(encoding)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dw2tPLmk2pEP"
   },
   "outputs": [],
   "source": [
    "def make_discriminator_encoder_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=(28, 28, 1)))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "      \n",
    "    model.add(tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "     \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_classifier():\n",
    "  model = tf.keras.Sequential()\n",
    "  \n",
    "  model.add(tf.keras.layers.Dense(1024, activation=tf.nn.leaky_relu))\n",
    "  model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gDkA05NE6QMs"
   },
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FMYgY_mPfTi"
   },
   "source": [
    "## Funciones de costo y optimizadores\n",
    "\n",
    "A continuación se definen las funciones de costo para ambos modelos, y los optimizadores a utilizar para minimizar dichas funciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jd-3GCUEiKtv"
   },
   "source": [
    "### Función de costo del generador\n",
    "La función de costo del generador consiste de una función de Sigmoid Cross Entropy sobre las imágenes generadas. Se busca que la salida del discriminador sea 1 (reales) para dichas imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90BIcCKcDMxz"
   },
   "outputs": [],
   "source": [
    "def generator_loss(generated_output):\n",
    "    return tf.losses.sigmoid_cross_entropy(tf.ones_like(generated_output), generated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PKY_iPSPNWoj"
   },
   "source": [
    "### Función de costo del discriminador\n",
    "\n",
    "La función de costo del discriminador tiene dos partes:\n",
    "1. Costo sobre imágenes reales: consiste de una Sigmoid Cross Entropy sobre las imágenes reales. Se busca que la salida del discriminador sea 1 (reales) para dichas imágenes.\n",
    "2. Costo sobre imágenes generadas: consiste de una Sigmoid Cross Entropy sobre las imágenes generadas por el generador. Se busca que la salida del discriminador sea 0 (falsas) para dichas imágenes.\n",
    "\n",
    "Luego se suman estas dos funciones para obtener la función de costo del discriminador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkMNfBWlT-PV"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, generated_output):\n",
    "    real_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=tf.ones_like(real_output), logits=real_output)\n",
    "    generated_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=tf.zeros_like(generated_output), logits=generated_output)\n",
    "    total_loss = real_loss + generated_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MgIc7i0th_Iu"
   },
   "source": [
    "### Optimizadores\n",
    "Se utiliza un optimizador Adam para cada modelo, ya que estos se entrenan en paralelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iWCn_PVdEJZ7"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "discriminator_optimizer = tf.train.AdamOptimizer(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mWtinsGDPJlV"
   },
   "source": [
    "## Checkpoints\n",
    "Los parámetros de ambos modelos se almacenarán como checkpoints durante el entrenamiento, para poder recuperarlos luego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CA1w-7s2POEy"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rw1fkAczTQYh"
   },
   "source": [
    "## Configuración del entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplos de validación\n",
    "Se mantendrá un conjunto de imágenes de validacion para visualizar los resultados del generador a lo largo del entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jylSonrqSWfi"
   },
   "source": [
    "### Paso de entrenamiento\n",
    "\n",
    "En cada paso del entrenamiento, el generador recibe un batch de imágenes enmascaradas, y regenera las secciones faltantes. Estas regiones generadas son luego insertadas en las imágenes del batch, completando las secciones faltantes con las regiones generadas por el generador. Luego, el discriminador recibe tanto estas imágenes con las secciones regeneradas, como imágenes reales.\n",
    "\n",
    "Luego, en base a los resultados del discriminador, se calculan las funciones de costo de ambos modelos. Y finalmente, se aplica un paso de los optimizadores para cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_image(patch, image):\n",
    "  \"\"\"\n",
    "  Apply the given patch to the image.\n",
    "  The patch is applied at the center of the image, assuming a 7x7 patch and a 28x28 image.\n",
    "  \"\"\"\n",
    "  # TODO: See if this could be done more efficiently.\n",
    "  \n",
    "  upper_edge = image[:, :10, :, :]\n",
    "  lower_edge = image[:, 17:, :, :]\n",
    "  \n",
    "  middle_left = image[:, 10:17, :10, :]\n",
    "  middle_right = image[:, 10:17, 17:, :]\n",
    "  \n",
    "  middle = tf.concat([middle_left, patch, middle_right], axis=2)\n",
    "  return tf.concat([upper_edge, middle, lower_edge], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3t5ibNo05jCB"
   },
   "outputs": [],
   "source": [
    "def train_step(full_images, full_reference_images, masked_images, masked_reference_images):\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    \n",
    "    generated_patches = generator((masked_images, masked_reference_images), training=True)\n",
    "    generated_images = patch_image(generated_patches, masked_images)\n",
    "    \n",
    "    real_output = discriminator((full_images, full_reference_images), training=True)\n",
    "    generated_output = discriminator((generated_images, masked_reference_images), training=True)\n",
    "    \n",
    "    gen_loss = generator_loss(generated_output)\n",
    "    disc_loss = discriminator_loss(real_output, generated_output)\n",
    "\n",
    "  gradients_of_generator = gen_tape.gradient(gen_loss, generator.variables)\n",
    "  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.variables)\n",
    "\n",
    "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.variables))\n",
    "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.variables))\n",
    "  \n",
    "  return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6TSZgwc2BUQ-"
   },
   "source": [
    "Debido a que ejecutar el entrenamiento de forma secuencial suele ser más lento que ejecutar el grafo de operaciones equivalente, utilizamos la función [tf.contrib.eager.defun](https://www.tensorflow.org/api_docs/python/tf/contrib/eager/defun) para generar el grafo de operaciones y así obtener una mejora en la performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iwya07_j5p2A"
   },
   "outputs": [],
   "source": [
    "train_step = tf.contrib.eager.defun(train_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceso completo de entrenamiento\n",
    "Aquí definimos el proceso completo de entrenamiento. Se itera sobre todo el dataset por cada epoch, y por cada batch del dataset se aplica un paso de entrenamiento. Luego de cada epoch, se muestran las imágenes de validación con las secciones regeneradas por el discriminador, y se grafican las funciones de costo de ambos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2M7LmLtGEMQJ"
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  gen_losses = []\n",
    "  disc_losses = []\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    for images in dataset:\n",
    "      (full_images, full_reference_images) = images[0]\n",
    "      (masked_images, unmasked_images, masked_reference_images) = images[1]\n",
    "      gen_loss, disc_loss = train_step(full_images, full_reference_images, masked_images, masked_reference_images)\n",
    "    \n",
    "    # Only store losses after each epoch\n",
    "    gen_losses.append(gen_loss)\n",
    "    disc_losses.append(disc_loss)\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "#     generate_and_save_images(generator,\n",
    "#                                epoch + 1,\n",
    "#                                validation_masked_images)\n",
    "    plot_losses(gen_losses, disc_losses)\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print ('Time taken for epoch {} is {} sec'.format(epoch + 1,\n",
    "                                                      time.time()-start))\n",
    "  # generating after the final epoch\n",
    "#   display.clear_output(wait=True)\n",
    "#   generate_and_save_images(generator,\n",
    "#                            epochs,\n",
    "#                            validation_masked_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(gen_loss, disc_loss): \n",
    "  plt.plot(range(1, len(gen_loss) + 1), gen_loss)\n",
    "  plt.plot(range(1, len(disc_loss) + 1), disc_loss)\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.title('Generator and Discriminator Losses')\n",
    "  plt.legend(['Gen loss', 'Disc loss'])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RmdVsmvhPxyy"
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(generator, epoch, masked_images):\n",
    "  # make sure the training parameter is set to False because we\n",
    "  # don't want to train the batchnorm layer when doing inference.\n",
    "  patches = generator(masked_images, training=False)\n",
    "  generated_images = patch_image(patches, masked_images)\n",
    "  \n",
    "  show_and_save_images(generated_images, 'image_at_epoch_{:04d}.png'.format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dZrd4CdjR-Fp"
   },
   "source": [
    "## Entrenamiento de la GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ly3UN0SLLY2l",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for epoch 1 is 104.48780131340027 sec\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for epoch 2 is 101.24153184890747 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-b740c71530db>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0;34m(\u001b[0m\u001b[0mfull_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_reference_images\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0;34m(\u001b[0m\u001b[0mmasked_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munmasked_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_reference_images\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0mgen_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_reference_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_reference_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Only store losses after each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-bda5c48f1250>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(full_images, full_reference_images, masked_images, masked_reference_images)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mgradients_of_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mgradients_of_discriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mgenerator_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients_of_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m    867\u001b[0m     flat_grad = imperative_grad.imperative_grad(\n\u001b[1;32m    868\u001b[0m         \u001b[0m_default_vspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m         output_gradients=output_gradients)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(vspace, tape, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m     61\u001b[0m   \"\"\"\n\u001b[1;32m     62\u001b[0m   return pywrap_tensorflow.TFE_Py_TapeGradient(\n\u001b[0;32m---> 63\u001b[0;31m       tape._tape, vspace, target, sources, output_gradients)  # pylint: disable=protected-access\n\u001b[0m",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" vs. \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m   \u001b[0msx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m   \u001b[0msy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m   \u001b[0mrx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_gradient_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m   \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mshape_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moptimize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m   7073\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m   7074\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7075\u001b[0;31m         _ctx._post_execution_callbacks, input, \"out_type\", out_type)\n\u001b[0m\u001b[1;32m   7076\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7077\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rfM4YcPVPkNO"
   },
   "source": [
    "**Restauración del último checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XhXsd0srPo8c"
   },
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4M_vIbUi7c0"
   },
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mLskt7EfXAjr"
   },
   "source": [
    "Luego del entrenamiento, visualizamos los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WfO5wCdclHGL"
   },
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5x3q9_Oe5q0A"
   },
   "outputs": [],
   "source": [
    "display_image(EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NywiH3nL8guF"
   },
   "source": [
    "### GIF del proceso de entrenamiento\n",
    "\n",
    "A continuación generamos un gif mostrando la evolución del generador sobre las imágenes de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IGKQgENQ8lEI"
   },
   "outputs": [],
   "source": [
    "with imageio.get_writer('mnist-inpainting.gif', mode='I') as writer:\n",
    "  filenames = glob.glob('image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  last = -1\n",
    "  for i,filename in enumerate(filenames):\n",
    "    frame = 2*(i**0.5)\n",
    "    if round(frame) > round(last):\n",
    "      last = frame\n",
    "    else:\n",
    "      continue\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)\n",
    "    \n",
    "# this is required in order to display the gif inside the notebook\n",
    "os.system('cp mnist-inpainting.gif mnist-inpainting.gif.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uV0yiKpzNP1b"
   },
   "outputs": [],
   "source": [
    "display.Image(filename=\"mnist-inpainting.gif.png\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dcgan.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
